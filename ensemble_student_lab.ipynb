{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ensemble Methods & Boosting — Student Lab\n",
        "\n",
        "Week 4 introduces sklearn models, but you must still explain *why* they work (bias/variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Dataset (synthetic default, real optional)\n",
        "\n",
        "### Task 0.1: Choose dataset\n",
        "Use synthetic by default. Optionally switch to breast cancer dataset.\n",
        "\n",
        "# TODO: set `use_real = False` or True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1400, 20)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensemble methods are going to combine multiple weaker models (e.g. decision trees) to create a stronger model. \n",
        "# Single models may overfit the training data, but by combining multiple models, ensemble methods can reduce overfitting and improve generalization.\n",
        "# Bagging(Bootstrap Aggregating) Random forest\n",
        "# Boosting eg = Gradient boosting\n",
        "# stacking ex: SVM, Random forest\n",
        "\n",
        "\n",
        "use_real = False  # TODO\n",
        "\n",
        "if use_real:\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "else:\n",
        "    X, y = make_classification(\n",
        "        n_samples=2000, # rows\n",
        "        n_features=20, # columns\n",
        "        n_informative=8,# these are those labels that are helping me decide how to predict the output/ If i just know these 8 columns, I can predict the output with good accuracy.\n",
        "        n_redundant=4, # these are those labels that are not helping me decide how to predict the output/ If i just know these 4 columns, I cannot predict the output with good accuracy.\n",
        "        class_sep=1.0, # this controls the separation between classes. Higher values make the classes more distinct and easier to classify.\n",
        "        flip_y=0.03, # this is the noise in the data, it will flip the labels of 3% of the data points, making it more challenging for the model to learn and generalize well.\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])\n",
        "Xtr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Baseline vs Trees vs Random Forest\n",
        "\n",
        "### Task 1.1: Train baseline decision tree vs random forest\n",
        "\n",
        "# TODO: Train:\n",
        "- DecisionTreeClassifier(max_depth=?)\n",
        "- RandomForestClassifier(n_estimators=?, max_depth=?, oob_score=True, bootstrap=True)\n",
        "\n",
        "Compute accuracy + ROC-AUC on validation.\n",
        "\n",
        "**Checkpoint:** Why does bagging reduce variance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tree (0.805, 0.8664611111111112)\n",
            "rf   (0.88, 0.9492277777777777)\n",
            "rf oob_score 0.9085714285714286\n"
          ]
        }
      ],
      "source": [
        "# Boosting : It's a core idea behind boosting is to build a strong model by sequentially training a series of weak models, where each subsequent model focuses on correcting the errors made by the previous models.\n",
        "# characteristics of boosting:\n",
        "# Sequential Learning: Boosting algorithms build models sequentially, with each model trying to correct the mistakes of the previous ones. This allows the ensemble to focus on difficult cases and improve overall performance.\n",
        "# Here we assign weights based on difficulty of the samples.\n",
        "# It typically uses simple models(like decision trees with limited depth) as weak learners, which are trained on weighted versions of the data.\n",
        "\n",
        "# Strengths of boosting:\n",
        "# 1.Improved Accuracy: Boosting can significantly improve the accuracy of predictions by combining multiple weak models into a strong ensemble.\n",
        "# 2. Can handle Complex patterns\n",
        "# 3. Reduces bias of weak learners\n",
        "\n",
        "\n",
        "# Weaknesses of boosting:\n",
        "# 1. Sensitive to outliers\n",
        "# 2. It can overfit\n",
        "# 3. Since its sequential, it can be slower to train than parallelizable methods like bagging.\n",
        "\n",
        "\n",
        "# Popular Boosting algorithms include:\n",
        "# 1. AdaBoost (Adaptive Boosting)\n",
        "# 2. Gradient Boosting\n",
        "\n",
        "\n",
        "# Baseline models: Means simplest reasonable model build to set a reference point.\n",
        "# Random forest: multiple decision trees trained on different subsets of the data and features\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
        "rf = RandomForestClassifier(n_estimators=300, max_depth=None, min_samples_leaf=2, oob_score=True, random_state=0, bootstrap=True, n_jobs=-1) \n",
        "# n_estimators=300 = looking for 300 decision trees, \n",
        "# oob(out of bag) evaluation that means it's going to use samples not seen by a tree as a validation data.\n",
        "#  bootstrap=True means each tree is trained on a random subset of the data with replacement, which is a key aspect of the random forest algorithm.\n",
        "# n_jobs=-1 means it will use all available CPU cores to train the trees in parallel, which can speed up the training process.\n",
        "\n",
        "\n",
        "tree.fit(Xtr, ytr)\n",
        "rf.fit(Xtr, ytr)\n",
        "\n",
        "def eval_model(clf, X, y): # I have data in X and its ans in y\n",
        "    pred = clf.predict(X) # I will use the model to predict the ans for the data in X and store it in pred\n",
        "    acc = accuracy_score(y, pred) # how well prediction performed by comparing it with the actual ans in y and store it in acc\n",
        "    # many sklearn classifiers have predict_proba; handle if not\n",
        "    if hasattr(clf, 'predict_proba'):\n",
        "        proba = clf.predict_proba(X)[:, 1]\n",
        "        auc = roc_auc_score(y, proba)\n",
        "    else:\n",
        "        auc = float('nan')\n",
        "    return acc, auc\n",
        "\n",
        "print('tree', eval_model(tree, Xva, yva))\n",
        "print('rf  ', eval_model(rf, Xva, yva))\n",
        "\n",
        "if hasattr(rf, 'oob_score_'):\n",
        "    print('rf oob_score', rf.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Feature importance gotcha\n",
        "\n",
        "Inspect `feature_importances_` and explain why correlated features can distort importances.\n",
        "\n",
        "# TODO: print top 10 features by importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "top idx [17 15  3 12  7 11  4 18 13  1]\n",
            "top importances [0.13533129 0.12294161 0.11825367 0.11781613 0.10023468 0.10020298\n",
            " 0.05578841 0.03412471 0.03289905 0.03011124]\n"
          ]
        }
      ],
      "source": [
        "# Correlated features : when i am getting same information but differ in units, for example height in cm and height in inches, they are correlated features.\n",
        "\n",
        "# TODO\n",
        "\n",
        "imp = rf.feature_importances_ # feature importance is a measure of how much each feature contributes to the predictions made by the model.\n",
        "top = np.argsort(-imp)[:10] # np.argsort(-imp) will sort the feature importances in descending order and return the indices of the sorted array. [:10] will give us the top 10 features based on their importance scores.\n",
        "print('top idx', top)\n",
        "print('top importances', imp[top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — Gradient Boosting\n",
        "\n",
        "### Task 2.1: Train GradientBoostingClassifier\n",
        "\n",
        "# TODO: Train GB with different n_estimators and learning_rate and compare.\n",
        "\n",
        "**Checkpoint:** Why can boosting overfit with too many estimators?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gb {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2} (0.8616666666666667, 0.9393722222222223)\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2} (0.8883333333333333, 0.9497444444444444)\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2} (0.8833333333333333, 0.9481888888888889)\n"
          ]
        }
      ],
      "source": [
        "# Gradient Boosting : Its powerful ML model, its going to use many weak models (may use many decision trees) to create a strong model. \n",
        "# Every time it fixes the mistakes of the previous model, it will give more weight to those samples that were misclassified by the previous model, so that the next model can focus on getting those right.\n",
        "# Each decision tree serves as a tutor to the next one and this process keeps on going until we have a strong model that can make accurate predictions.\n",
        "\n",
        "# I start with a simple decision tree and then I keep on adding more trees to fix the mistakes of the previous trees, and this process continues until I have a strong model that can make accurate predictions.\n",
        "\n",
        "# Key Characteristics of Gradient Boosting:\n",
        "# 1. Both classification and regression\n",
        "# 2. Handle non-linear relationships\n",
        "# 3. Strong performance on structured, tabular data\n",
        "\n",
        "# Can overfit if not tuned properly.\n",
        "# Slower to train\n",
        "\n",
        "\n",
        "settings = [\n",
        "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2},\n",
        "]\n",
        "\n",
        "for s in settings:\n",
        "    gb = GradientBoostingClassifier(random_state=0, **s)\n",
        "    gb.fit(Xtr, ytr) # Xtr is the training data and ytr is the training labels, we are fitting the model on the training data.\n",
        "    print('gb', s, eval_model(gb, Xva, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — XGBoost-style knobs (conceptual)\n",
        "\n",
        "### Task 3.1: Explain what each knob does\n",
        "Write 2-3 bullets each:\n",
        "- subsample\n",
        "- colsample\n",
        "- learning rate\n",
        "- max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **subsample: Subset of a sample**\n",
        "- **colsample: Fraction of features used in each tree**\n",
        "- **learning_rate: How much each new tree is going to contribute**\n",
        "- **max_depth: Maximum Depth of Each Tree And it tells me how complex each tree can be**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Baseline vs RF vs GB compared\n",
        "- OOB score discussed (if available)\n",
        "- Feature importance gotcha explained"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
